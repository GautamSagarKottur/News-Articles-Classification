{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used Google Colab for creating this script. \n",
    "#To run this without errors.Please change path in cells which have this comment: \"Includes google drive path\"\n",
    "#All the following packages are needed to run this script without errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "fgcybd6LYxGp",
    "outputId": "8182b6e4-00f2-4185-a5da-5056cec473c0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gensim.models as g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XQ4JFKa6Y3t-",
    "outputId": "74ac412d-f837-4c38-92f2-03b1fb97eb68"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FcWNpTd0nJQj"
   },
   "outputs": [],
   "source": [
    "#Includes google drive path.\n",
    "#Function: reads csv and prints shape, null values summary column-wise. \n",
    "def read_df(df_name):\n",
    "    df = pd.read_csv('gdrive/My Drive/News Articles Dataset/'+ df_name +'.csv', header = None)\n",
    "    print('Shape:',df.shape)\n",
    "    print('Null values')\n",
    "    print(df.isnull().any())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "kG8W5JahYxGw",
    "outputId": "0d342d38-2dcb-48ee-c0c4-7e1aef0813f9"
   },
   "outputs": [],
   "source": [
    "df = read_df('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Zr9b6n_YxHA"
   },
   "outputs": [],
   "source": [
    "#Function: renames and combines headline and content\n",
    "def rename_combine_cols(df, col1, col2, col3, newcol):\n",
    "    new_df = df.rename(columns = {0:col1, 1:col2, 2:col3})\n",
    "    new_df[newcol] = new_df[col2] + ' ' + new_df[col3]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "haCVX8GdYxHH"
   },
   "outputs": [],
   "source": [
    "df = rename_combine_cols(df, 'class', 'headline', 'content', 'combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "-Xt9YLRbYxHM",
    "outputId": "8d183f0c-fc6a-4740-fb1d-c70d6b345b98",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['class'].value_counts().sort_values().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "mEo0G024YxHQ",
    "outputId": "1916ce1c-9353-47e9-dbd7-bde0c9cb4d8a"
   },
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5lvxKHcRYxHm"
   },
   "outputs": [],
   "source": [
    "#Function: removes special characters, punctuations and numbers; tokenize; lemmatize and remove stop words \n",
    "def text_preprocess(text):\n",
    "    text_stripwhitespaces = text.strip()\n",
    "    text_stripwhitespaces = text_stripwhitespaces.replace(\"\\\\\", \" \")\n",
    "    text_clean =  re.sub('[~`!@#$%^&*():;\"{}_/?><\\|.,`0-9]', '', text_stripwhitespaces.replace('-', ' '))\n",
    "    tokens = word_tokenize(str(text_clean).lower())\n",
    "    #words = [lemmatizer.lemmatize(word) for word in tokens if not word in stop_words]\n",
    "    words = [word for word in tokens if not word in stop_words]\n",
    "    if words[0] == words[-1]:\n",
    "        words.pop(-1)\n",
    "    final_text = ' '.join(words)\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "am7iaJe9YxHq"
   },
   "outputs": [],
   "source": [
    "df['combined'] = df['combined'].apply(lambda x: text_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vZNSURxJYxIg",
    "outputId": "914968aa-b72b-4ce5-800e-ba47c655dd89"
   },
   "outputs": [],
   "source": [
    "result = Counter(\" \".join(df['combined'].values.tolist()).split(\" \")).items()\n",
    "x = sorted(list(result), key = lambda x: x[1])\n",
    "print('No.of words occuring more than 100 times:', len([i[1] for i in x if i[1] >= 100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66hpFaEqYxIU"
   },
   "outputs": [],
   "source": [
    "text = df['combined'].iloc[:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IgKEaWw0q5-V"
   },
   "source": [
    "TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V18KlQHGYxIx"
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(max_features = 5000, ngram_range = (1, 3), max_df = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7QpqCcHQYxI6"
   },
   "outputs": [],
   "source": [
    "x_train = vec.fit_transform(text).toarray()\n",
    "y_train = df['class'].iloc[:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scY08RpOFQAU"
   },
   "source": [
    "Preparing Test Data: Treating the test data with same pre-processing steps as train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "tso41oDcjx3f",
    "outputId": "5662c043-9817-4e7e-977f-05766ce50696"
   },
   "outputs": [],
   "source": [
    "df1 = read_df('test')\n",
    "df1 = rename_combine_cols(df1, 'class', 'headline', 'content', 'combined')\n",
    "df1['combined'] = df1['combined'].apply(lambda x: text_preprocess(x))\n",
    "text = df1['combined'].iloc[:].values\n",
    "x_test = vec.transform(text).toarray()\n",
    "y_test = df1['class'].iloc[:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AfMsqdwEinH"
   },
   "outputs": [],
   "source": [
    "#Function: trains model, predicts on train data and test data, gets training and testing accuracy\n",
    "def train_fit_predict(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train,y_train)\n",
    "    y_trainpred = model.predict(x_train)\n",
    "    print('Training Accuracy: ', accuracy_score(y_train, y_trainpred))\n",
    "    y_testpred = model.predict(x_test)\n",
    "    score = accuracy_score(y_test, y_testpred)\n",
    "    print('Training Accuracy: ', accuracy_score(y_test, y_testpred))\n",
    "    return y_test, y_testpred, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTxvT698q-_g"
   },
   "source": [
    "Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IcS0T2oBYxJD"
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Ow4HpDIZEnVl",
    "outputId": "d792be41-285f-4d58-d884-ea4301ba8bbc"
   },
   "outputs": [],
   "source": [
    "y_test, y_testpred, model = train_fit_predict(model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpDHanqbcAbI"
   },
   "outputs": [],
   "source": [
    "#Function: Prints and plots confusion matrix as heatmap \n",
    "def plot_conf_matrix(y, yhat, model):\n",
    "    conf_matrix = confusion_matrix(y.tolist(), yhat.tolist(), labels=[1,2,3,4])\n",
    "    print(conf_matrix)\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, range(1,5), range(1,5))\n",
    "    plt.figure(figsize=(10,7))\n",
    "    ax = plt.axes()\n",
    "    sns.heatmap(conf_matrix_df,annot=True,fmt='g',cmap='Blues') # font size\n",
    "    ax.set_title(str(model))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "colab_type": "code",
    "id": "ouoUPKUFf508",
    "outputId": "d35e17d2-83db-4e99-b15e-e3930e5723ab"
   },
   "outputs": [],
   "source": [
    "plot_conf_matrix(y_test, y_testpred, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WCb2II3yXsnm"
   },
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPwGGhcXXxd3"
   },
   "outputs": [],
   "source": [
    "logr = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hbfxeWLQY9YH",
    "outputId": "9033a3b4-283d-4a0d-9708-c9f9c0bf5dd5"
   },
   "outputs": [],
   "source": [
    "y_test, y_testpred, model = train_fit_predict(logr, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "colab_type": "code",
    "id": "ZsvfaL4-HRsU",
    "outputId": "41bde056-f6cf-475d-8587-3fd94c53312e"
   },
   "outputs": [],
   "source": [
    "plot_conf_matrix(y_test, y_testpred, logr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6d4frhqGQLkf"
   },
   "source": [
    "Doc2Vec Vectorization using pre-trained doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-trained Doc2Vec model trained on Associated Press News articles\n",
    "#To verify; model has to be downloaded. Download link: https://github.com/jhlau/doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Includes google drive path\n",
    "d2v_path = 'gdrive/My Drive/doc2vec_pretrained/doc2vec.bin'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "B9BXmMzAIma5",
    "outputId": "7b049889-6813-4ab8-c289-adac847dd74a"
   },
   "outputs": [],
   "source": [
    "d2v = g.Doc2Vec.load(d2v_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddr3cMaNImXW"
   },
   "outputs": [],
   "source": [
    "#vectorizing train combined text using pre-trained doc2vec model\n",
    "combinedtext_doc2vec = [d2v.infer_vector(i.split()) for i in df.combined.tolist()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wz572iT7LfY5"
   },
   "outputs": [],
   "source": [
    "x_train_d2v = np.vstack(combinedtext_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWITamU0Lq3-"
   },
   "outputs": [],
   "source": [
    "np.save('gdrive/My Drive/x_train_d2v.npy', x_train_d2v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHwZ257EOsVB"
   },
   "outputs": [],
   "source": [
    "#vectorizing test combined text using pre-trained doc2vec model\n",
    "combinedtext_doc2vec1 = [d2v.infer_vector(i.split()) for i in df1.combined.tolist()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iy3F8z-LPQ06"
   },
   "outputs": [],
   "source": [
    "x_test_d2v = np.vstack(combinedtext_doc2vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kregNejP17L"
   },
   "outputs": [],
   "source": [
    "np.save('gdrive/My Drive/x_test_d2v.npy', x_test_d2v) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4uHmC1HDRdIC"
   },
   "source": [
    "By vectorizing our text corpus using doc2vec, we have reduced or feature dimensions from 5000 to 300. TFIDF feature vector was a big sparse matrix. But now we have a dense, tightly packed feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2R3I5ElQZ9Q"
   },
   "source": [
    "Naive Bayes on Doc2vec features of the news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jmOIvJsoP3l0"
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BYUs5RViQYYu",
    "outputId": "f800b2b6-18d3-4109-b75d-e911ad649853"
   },
   "outputs": [],
   "source": [
    "y_test, y_testpred, model = train_fit_predict(model, x_train_d2v, y_train, x_test_d2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "colab_type": "code",
    "id": "fiMLoPUAQuja",
    "outputId": "c7ad7f3a-2d69-4bda-87e0-2afbb3488651"
   },
   "outputs": [],
   "source": [
    "plot_conf_matrix(y_test, y_testpred, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LzOHfn8BRHht"
   },
   "source": [
    "Logistic Regression on Doc2vec features of the news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DZ0a9HIpRMCh",
    "outputId": "c517d104-afca-44d6-ff57-cdaac14ddee1"
   },
   "outputs": [],
   "source": [
    "y_test, y_testpred, model = train_fit_predict(logr, x_train_d2v, y_train, x_test_d2v, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "colab_type": "code",
    "id": "a7l-QaHeRWYu",
    "outputId": "cee00153-5f85-4533-eaa6-17d2d95decdc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_conf_matrix(y_test, y_testpred, logr)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GautamSagarKottur_NewsArticlesClassification_Assignment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
